SECTION 1: PROJECT OVERVIEW AND PURPOSE

Title: Medical X-Ray Analysis using CNN and Grad-CAM

This project develops a research-grade, explainable AI system for automated medical X‑ray image analysis. The system implements convolutional neural networks (CNNs) to classify multiple radiological conditions and provides interpretable visual explanations through Gradient-weighted Class Activation Mapping (Grad‑CAM). The end-user application is implemented as an interactive Streamlit interface with role-based access, report generation, feedback capture, and configurable visualization parameters.

The problem addressed is the known variability and workload constraints associated with manual medical image interpretation. Radiographs require expert reading; triage pressure and subtle features (e.g., fine lucencies in fractures, borderline cardiac silhouettes) can be challenging in real time. This system aims to support, not replace, clinical decision-making by offering consistent automated classification with localized visual evidence. Application domains included in this build are limb bone fracture, chest pneumonia, knee osteoporosis, knee osteoarthritis (arthritis), and cardiomegaly. All modules, metrics, and outcomes documented here are for academic and research usage; this software is not cleared for clinical deployment.


SECTION 2: SCIENTIFIC AND TECHNICAL BACKGROUND

Convolutional Neural Networks (CNNs) are the state-of-the-art paradigm for image analysis. By learning spatially local receptive fields and hierarchical feature representations, CNNs can detect edges, textures, and higher-level radiographic patterns. Transfer learning with pre-trained backbones (e.g., DenseNet121 and MobileNetV2) is especially effective in medical imaging where labeled data may be limited. DenseNet architectures utilize dense connectivity to promote feature reuse and efficient gradient flow, while MobileNetV2 provides lightweight depthwise separable convolutions for fast inference.

Grad‑CAM provides post-hoc explainability by backpropagating gradients of the class score with respect to intermediate activation maps of a chosen convolutional layer. Specifically, global average pooled gradients weight the spatial activation maps; a rectified linear unit (ReLU) produces a non-negative saliency map highlighting regions most influential for the model’s decision. In practice, Grad‑CAM is widely used to verify if a model’s attention aligns with clinically relevant anatomy or pathology.

Explainability is essential in healthcare AI to promote clinician trust, enable model debugging, and provide an auditable rationale for predictions. Literature on DenseNet and ResNet families has shown strong performance across medical tasks, but regulatory and clinical acceptance require interpretable outputs. Grad‑CAM, along with derivatives (Grad‑CAM++, Score‑CAM), addresses this requirement and facilitates human-in-the-loop validation.


SECTION 3: DATASET DESCRIPTION AND PREPROCESSING

This project integrates multiple dataset sources captured in `dataset_info.json`. The datasets include bone fracture X‑rays (e.g., MURA‑style limb series organized under `Dataset/ARM/MURA_Organized/Forearm`), chest X‑ray pneumonia (`Dataset/CHEST/Pneumonia_Organized`), cardiomegaly (`Dataset/CHEST/cardiomelgy/train/train`), knee osteoarthritis (`Dataset/KNEE/Osteoarthritis/Combined_Osteoarthritis_Dataset`), and knee osteoporosis (`Dataset/KNEE/Osteoporosis/Combined_Osteoporosis_Dataset`). Each dataset is configured for binary classification with class labels such as Normal vs Condition (e.g., Fracture/Normal, Pneumonia/Normal). File formats primarily include JPEG/PNG, with optional DICOM support via `pydicom` when clinical images are provided.

Preprocessing is centralized in `utils/image_preprocessing.py`. Images are loaded, converted to RGB as needed, resized to a default target of 224×224 pixels, and normalized to [0,1]. Formally, per-pixel normalization follows x_norm = x / 255.0. The module supports optional contrast enhancement and data augmentation. Augmentation pipelines include rotation (±10–20°), horizontal flip, brightness/contrast adjustments, zoom, and noise—implemented either via `albumentations` (if available) or a basic augmentation fallback. The augmentation ranges align with the code: rotation up to 10–20°, brightness ±10–20%, and random flips.

During training, images are batched by the Keras generators or pipelines; representative batch sizes observed in repository scripts include 16–32 for standard/intensive training and 25 for quick CPU‑optimized runs (see `train_quick_5epoch_models.py`). At inference, the system uses batch size 1, as preprocessing functions add a batch dimension to the single uploaded image. For DICOM files, pixel arrays are rescaled to the 0–255 range before conversion; while the system can parse DICOM, metadata handling is minimal and privacy best practices (de-identification) are recommended prior to ingestion.


SECTION 4: MODEL ARCHITECTURE AND DESIGN

The primary backbone is DenseNet121, with MobileNetV2 used for “Fast” models. In transfer learning configurations, the backbone is loaded with ImageNet weights and the classification head is replaced with GlobalAveragePooling, BatchNormalization, fully connected layers, Dropout, and a final sigmoid (binary) or softmax (multi-class) output. Input shape is typically 224×224×3 for DenseNet121; MobileNetV2 “Fast” models use 128×128×3 for speed.

Layer-wise design in `utils/model_inference.py` dummy builders and training scripts is representative: Input → Conv/BN/ReLU blocks → Pooling → GlobalAveragePooling2D → Dense(ReLU) + Dropout → Output(Dense with sigmoid). Activation functions include ReLU in intermediate layers and sigmoid for binary outputs. Typical kernel size is 3×3 for convolutional layers. Regularization is performed through Dropout (e.g., 0.3–0.5), and BatchNormalization is used to stabilize training. Optimizer is Adam; learning rates range from 1e‑3 (quick) to 1e‑5 (intensive). Losses are binary cross‑entropy for binary classification and categorical/sparse categorical cross‑entropy when multi-class is used.

Parameter counts vary by architecture: DenseNet121 backbones plus small heads generally yield ~7–8 million parameters, while MobileNetV2 “Fast” variants can be ~1.4–2.0 million parameters, as referenced in `models/registry/model_registry.json`. A model registry (`models/registry/model_registry.json`) defines available models, metadata, performance metrics, and an `active_models` map that controls which model loads per condition at runtime. The system dynamically loads active models via `utils/model_inference.ModelManager`, with fallbacks to dummy CNNs (`_create_dummy_model` and `_create_simple_dummy_model`) if a model file is missing or incompatible, ensuring the app remains operable.


SECTION 5: TRAINING PROCEDURE AND EXPERIMENTAL SETUP

Training recipes in the repository cover quick, standard, and intensive regimens. The quick 5‑epoch pipeline (`train_quick_5epoch_models.py`) uses DenseNet121 with batch size 25, validation split 0.2, and Adam(1e‑3). Callbacks include `ModelCheckpoint` (best on `val_accuracy`), `ReduceLROnPlateau` (factor 0.5, patience 2), and `EarlyStopping` (patience 3). The advanced trainer in `utils/model_trainer.py` defines three settings: quick_test (5 epochs, batch size 16, lr 1e‑3), standard (25 epochs, batch size 32, lr 1e‑4), and intensive (50 epochs, batch size 16, lr 1e‑5). For cardiomegaly, the trainer unfreezes more layers and uses gentler learning-rate reduction and higher early‑stopping patience (up to 15) due to task difficulty.

Hardware depends on local environment; the project runs on CPU or GPU. GPU acceleration (CUDA/cuDNN) is recommended for intensive training, while quick tests are feasible on CPU. During training, Keras’ `ImageDataGenerator` performs on-the-fly augmentation (rescale=1/255, rotation, shifts, flips). Training progress is tracked via Keras callbacks; the advanced trainer adds TensorBoard logs and can produce confusion matrices and classification reports using `sklearn`.

Observed results stored in the registry indicate test accuracies for the latest intensive and fast models: Pneumonia (DenseNet121 intensive) ~0.958, Arthritis (DenseNet121 intensive) ~0.942, Osteoporosis (DenseNet121 intensive) ~0.918, Bone Fracture (DenseNet121 intensive) ~0.73, and Cardiomegaly (DenseNet121 intensive) ~0.63. Fast MobileNetV2 variants report test accuracies such as Arthritis Fast ~0.9703, Pneumonia Fast ~0.8719, Osteoporosis Fast ~0.869, Bone Fracture Fast ~0.7704, and Cardiomegaly Fast ~0.6562. These values provide realistic baselines for subsequent ablations and hyperparameter tuning.


SECTION 6: INFERENCE PIPELINE AND SYSTEM WORKFLOW

The end-to-end workflow in the Streamlit application (`app.py`) proceeds as follows: users upload an image; preprocessing standardizes size and scale (`utils/image_preprocessing.preprocess_image` and optional `apply_augmentation` for experimentation); the active model for the selected condition loads via `utils/model_inference.load_models()` or `ModelManager.load_model()`; predictions are generated; Grad‑CAM heatmaps are computed (`utils/gradcam.generate_gradcam_heatmap`); and the overlay is displayed along with confidence and optional clinical guidance (e.g., knee recommendations). Reports can be exported to PDF/HTML using `utils/report_generator`.

Prediction outputs are probabilities (sigmoid) with thresholding at 0.5 by default, presented as a binary label alongside confidence. Performance optimizations include model caching via `@st.cache_resource`, registry-driven loading paths, and Keras’ compile=False and `safe_mode=False` for fast deserialization followed by explicit recompilation to a consistent runtime loss/metrics set. The `SettingsManager` provides user-configurable parameters such as Grad‑CAM intensity and confidence thresholds.

A simplified text flowchart of inference: (1) Upload → (2) Preprocess (resize 224×224, normalize) → (3) Load appropriate active model → (4) Predict p(y|x) → (5) Compute Grad‑CAM heatmap on target conv layer → (6) Overlay heatmap with alpha blending; optionally detect fracture regions → (7) Display results; export report; log usage/feedback.


SECTION 7: GRAD‑CAM IMPLEMENTATION AND VISUALIZATION

The Grad‑CAM implementation in `utils/gradcam.py` is robust to nested model architectures common in transfer learning (Sequential wrapper containing a Functional backbone). The class `GradCAM` automatically identifies the target layer by scanning for the last 4D activation/conv layer, skipping non-spatial layers (dense/global/dropout). If a nested base model (e.g., DenseNet121 or MobileNetV2) is detected, the algorithm locates the layer within the backbone. A gradient model is constructed that outputs both the target layer feature maps and the final predictions, enabling gradient computation of the selected class score with respect to activations.

Mathematically, Grad‑CAM forms weights α_k by global average pooling the gradients of the class score y^c with respect to activation maps A^k (α_k = mean_{i,j} ∂y^c/∂A^k_{ij}). The class activation map is L^c = ReLU(Σ_k α_k A^k). The code normalizes the heatmap, rescales it to the input image size, and overlays it using a colormap and opacity parameter. Fallbacks exist when gradients are unavailable or the target layer has inadequate spatial resolution: an attention‑style gradient w.r.t. input map or a synthetically generated heatmap ensures the UI presents an interpretable visualization rather than failing silently.

The top-level convenience function `generate_gradcam_heatmap(model, image_array, original_image, class_index=None, model_type='bone', intensity=0.4)` returns a PIL image with the overlay. The `intensity` parameter (alpha) controls blending, and the system optionally draws bounding boxes for high‑attention fracture candidates via contour detection when `model_type=='bone'`. In practice, high‑intensity regions correlate with suspected pathology (e.g., cortical discontinuities in fractures), which clinicians can compare against radiographic anatomy. [Figure 1] in a full report would illustrate an example overlay and interpretation notes.


SECTION 8: USER INTERFACE AND ROLE MANAGEMENT

The user interface in `app.py` is implemented with Streamlit and enhanced CSS for dark‑mode readability. The sidebar presents navigation options (e.g., Classification, Model Information, Advanced Features), while the main panel organizes upload, analysis, and visualization components. The UI exposes settings for Grad‑CAM intensity and displays prediction confidence along with the overlay. Documentation and usage metrics can be surfaced for transparency and educational use.

Role-based access is implemented in `utils/authentication.py` with a JSON‑backed user store (`user_data.json`). Demo accounts include admin/doctor/student profiles. Permissions vary by role: students can view results and model information; doctors/radiologists are granted broader access to advanced features and batch operations. Persistent preferences and system behavior are maintained via `utils/settings_manager.SettingsManager`, which stores `settings/user_settings.json` with backup/restore and import/export capabilities.


SECTION 9: SOFTWARE AND HARDWARE REQUIREMENTS

The project targets Python 3.x on Windows, macOS, or Linux, with this workspace configured on Windows. Core dependencies include TensorFlow/Keras 2.x, Streamlit, NumPy, OpenCV, Pillow, and Matplotlib; optional packages are `scipy` (heatmap smoothing), `albumentations` (advanced augmentation), and `pydicom` (DICOM support). A GPU with CUDA/cuDNN is recommended for intensive training; inference is feasible on CPU.

Environment setup typically involves creating a virtual environment and installing packages pinned to the TensorFlow version supported by the local hardware. While a unified requirements file is not enumerated here, the code imports identify necessary dependencies. The application can be launched with Python and the Streamlit runner; models are loaded from `models/<condition>/...` as declared in the registry.


SECTION 10: RESULTS AND PERFORMANCE ANALYSIS

Registry-reported test accuracies provide a quantitative baseline across conditions. DenseNet121 “Intensive” models: Pneumonia ~0.958, Arthritis ~0.942, Osteoporosis ~0.918, Bone Fracture ~0.73, Cardiomegaly ~0.63. Fast MobileNetV2 variants prioritize speed, with representative test accuracies: Arthritis Fast ~0.9703, Pneumonia Fast ~0.8719, Osteoporosis Fast ~0.869, Bone Fracture Fast ~0.7704, Cardiomegaly Fast ~0.6562. These values indicate strong performance on easier tasks (pneumonia, arthritis) and room for improvement on cardiomegaly and fracture detection.

Qualitatively, Grad‑CAM overlays often focus on salient anatomical regions (e.g., lung fields for pneumonia, joint space and cortical margins for knee pathologies, distal radius/ulna or metacarpals for forearm fractures). Where performance is lower, heatmaps may highlight wide or diffuse regions, suggesting insufficient feature specificity or dataset variability. Confusion trends typical of chest tasks include false positives related to prominent vascular markings or exposure variance; for fractures, overlapping soft tissue or projection differences can confound the model. The advanced trainer’s use of classification reports and confusion matrices supports structured error analysis.


SECTION 11: ETHICAL, SAFETY, AND COMPLIANCE NOTES

This software is intended for research and education only and is not a medical device. Any clinical interpretation remains the responsibility of qualified healthcare professionals. The system’s predictions should not be used as the sole basis for diagnosis or patient management decisions.

Data privacy and compliance are paramount when handling medical images. Users should ensure de‑identification of DICOM headers and adherence to applicable standards and regulations (e.g., HIPAA in the US, GDPR in the EU) in their data pipelines. The repository demonstrates technical capability but does not enforce legal compliance; deployments must incorporate institutional policies, audit logging, and access controls.


SECTION 12: LIMITATIONS AND FUTURE SCOPE

Limitations include dataset diversity, especially for challenging tasks (cardiomegaly, subtle fractures), and potential domain shift across institutions and imaging protocols. Computational constraints may limit intensive training on CPU‑only systems. While Grad‑CAM provides coarse localization, it is not a substitute for pixel‑accurate lesion segmentation.

Future enhancements include integrating Grad‑CAM++ or Score‑CAM for improved saliency, adding multi‑label classification where conditions co‑occur, and introducing segmentation heads for precise localization. Reinforcement learning or active learning could prioritize informative samples for labeling or retraining. Cloud deployment with hospital system integration and formal explainability benchmarking (e.g., reader studies with radiologists) would strengthen translational impact and trust.


SECTION 13: SUMMARY OF FILE STRUCTURE AND MODULES

The main application (`app.py`) orchestrates the UI, routing, and interaction with utilities. It calls preprocessing functions (`utils/image_preprocessing.preprocess_image`, `apply_augmentation`), model loaders (`utils/model_inference.load_models`, `ModelManager.load_model`), and Grad‑CAM (`utils/gradcam.generate_gradcam_heatmap`). It also integrates report generation (`utils/report_generator`) and feedback/usage logging.

`utils/gradcam.py` contains the `GradCAM` class with target‑layer discovery resilient to nested backbones. It exposes `generate_heatmap`, `create_superimposed_image`, and a high-level `generate_gradcam_heatmap` wrapper with the `intensity` parameter. `utils/model_inference.py` defines `ModelManager` for registry synchronization (`models/registry/model_registry.json`), active model selection, caching, and robust fallbacks via `_create_dummy_model`. `utils/image_preprocessing.py` implements loading, resizing, normalization, augmentation, quality checks, and optional DICOM handling. `utils/authentication.py` provides role-based login and permissions backed by `user_data.json`. `utils/settings_manager.py` persists user/system settings with backup/restore and import/export support, enabling reproducible configurations.

Inter-module dependencies are clear: the app drives data flow; preprocessing standardizes inputs; inference modules select and run the correct model; Grad‑CAM builds visual explanations; settings and auth govern runtime behavior; and reporting/feedback modules document outcomes. The `models/registry` folder provides metadata and performance references to ensure the UI displays model information accurately.


SECTION 14: CONCLUSION

This project delivers an explainable, modular AI system for medical X‑ray classification across multiple conditions. It couples strong CNN backbones with Grad‑CAM visualizations to provide interpretable outputs, embedded within an accessible Streamlit interface. The design emphasizes robustness via dynamic registry‑based loading, safe fallbacks, and configurable settings.

The documented training setups and registry results establish credible baselines, while the Grad‑CAM pipeline ensures clinical interpretability remains central. With further validation, dataset expansion, and integration of advanced explainability methods, this platform can serve as a scalable foundation for research, education, and eventual translational studies in computer‑assisted radiographic diagnosis.
